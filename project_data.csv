Title,Content
DIAGNOSIS OF HEART DISEASE USING ADAPTIVE NETWORK-BASED,DIAGNOSIS HEART DISEASE USING ADAPTIVE NETWORKBASED FUZZY INFERENCE SYSTEM ANFIS ERIN KUNZ CSSPRINGABSTRACT Heart Disease leading cause death both women United States Approximatelypeople from heart disease each year which equivalent toin everydeaths Many health risk lifestyle factors have been associated with onset heart disease including high blood pressure high cholesterol smoking physical activity levels sleep quality stress much more Most prevention treatment techniques focused these such lifestyle factors this reason diagnosis prognosis somewhat subjective despite clinically available quantifications that more indicative heart disease this study variety machine learning techniques used predict occurrence heart disease given variety easily obtainable hearthealthrelated features Furthermore incorporate more subjective nonlinear decisionmaking that medical professional might offer likelihood that heart disease prediction involve complex relationships between features further analysis utilizing fuzzy inference system done INTRODUCTION RELATED WORK More thanAmericans from heart disease every year leading cause death globally more common been increasing many developed countries particularly United States variety risk factors linked heart disease including diet exercises tobacco alcohol high blood pressure estimated that % heart disease preventable through improving these risk factors Cardiovascular heart disease actually refers class diseases that involve heart blood vessels includes several distinct types – coronary artery diseases stroke heart failure hypertensive heart failure rheumatic heart disease many more this reason diagnosis even less linear based lifestyle factors available medical data Being able better predict prevent heart disease could change save lives millions people variety machine learning classification techniques able better analyze more complex relationships between medical data features possible heart disease diagnosis Several studies have been completed define classifier heart disease utilizing machine learning techniques init been proven that machine learning techniques such logistic regression neural networks improve upon established risk prediction methods diagnosing heart disease Ultimately prediction methods achieved accuracy nearing % While great improvement from current medical standard with heart disease affecting such high portion population misdiagnosis rate higher than % still huge portion population improving this accuracy rate even further could positively impact possibly save many more lives Additionally many studies have been done predicting diagnosis other lifestylecorrelated illnesses utilizing fuzzy logic principles Inference systems utilizing fuzzy logic have been used with success predictions both Diabetes Hepatitis Intuitively continuous scale betweenandmakes sense diseases like heart disease diabetes hepatitis known gradual development result longterm lifestyle factors usually aren’instantaneous occurrences this study input willfeatures medical data that easily obtainable typically used heart disease diagnosis output will prediction heart diseaseor heart diseaseWe will tune variety classification algorithms achieve best accuracy possible Furthermore this study also attempt incorporate nonlinear complexities through merging fuzzy logic principles with artificial neural network Adaptive NetworkBased Fuzzy Inference System seeking further improvement DATASET FEATURES this study datasets were considered first complete dataset ofsubjects withfeatures including chest pain type resting blood pressure cholesterol fasting blood sugar resting electrocardiographic results exercise induced angina depression induced exercise relative rest slope peak exercise segment number major vessels colored flourosopy presence defect reversible defect fixed defect diagnosis heart diseaseorThe second dataset ofsubjects with same firstfeatures first dataset diagnosis heart disease This dataset collected from studies Hungary Switzerland Cleveland Both these datasets were obtained from Machine Learning RepositoryAny missing feature entries datasets were imputed with mean values Initially looking correlation matrix dataset withfeatures dataset withfeatures appears there strong correlation between feature diagnosis heart disease target Figure : Correlation matrix forfeatures combined datasets left correlation matrix allfeatures first dataset right METHODS EXPERIMENTS RESULTS Four types binary classifiers were initially tested both datasets: logistic regression KNearest Neighbor classifier Support Vector Machine classifier artificial neural network experiments data randomly shuffled normalized split % training % testing accuracies reported test data LOGISTIC REGRESSION Initially logistic regression applied both datasets Logistic regression nonlinear transformation linear regression model between input variables binary class assignment maps this linear regression using function like sigmoid shape that probabilities mapped betweenandThen class input example determined based this probability value greater thanwould denote classand vice versa first phase classifier both datasets logistic regression strategy fairly well with over % accuracy both datasets larger dataset withfeatures achieved accuracy ofand smaller dataset withfeatures achieved accuracy ofThere nearly % improvement between larger dataset with less features than smaller dataset with more features possibly indicating that three additional features provide additional relevant information diagnosis heart disease However smaller size dataset more experiments will need confidently make this conclusion this could also result overfitting this particular dataset region collection KNEIGHBORS CLASSIFIER Next Knearest neighbors classifier both datasets this type classifier nearest neighbors given data point analyzed majority class these neighbors assigned data point number nearest neighbors varied this study varied fromtofor larger dataset finding peak accuracy ofat= nearest neighbors smaller dataset varied fromtofinding peak accuracy offor= nearest neighbors both datasets accuracy classifier improved from logistic regression classifier Again slightly superior performance from smaller dataset with more features than larger dataset with three fewer features Figure : Score plots variations neighbors small dataset larger dataset bottom SUPPORT VECTOR MACHINES Next support vector machine classifier tested accuracy could further improved Support vector machines machine learning technique that looks separations data when mapped higherdimensional space utilize variety “kernel tricks” assess just linear relationships also polynomial radial basis function sigmoid feature mappings find split higher dimensional space between binary classifications this study linear polynomial radial basis function sigmoid feature mappings were tested both datasets Figure : Score plots different feature mapping classifier large dataset left smaller dataset right classifier does improve upon theoraccuracy from highest performing Kneighbors algorithm however interesting note that different feature mappings different performance orders datasets first time classifier larger dataset with fewer features outperformed classifier smaller dataset with more features radial basis function feature large dataset classifier performed best overall with accuracy ofwhile sigmoid function performed best smaller dataset with accuracy ofThis indicates that three additional features smaller dataset complexities relationships between classes higher dimensional space that able entirely captured these four feature mapping split strategies NEURAL NETWORK next experiment implement neural network classifier datasets could exceed previous accuracy ofSeveral iterations neural network were constructed varying numbers hidden layers numbers neurons each layer batch size during training hidden input layers used ReLu activation function output layer employed sigmoid activation function both datasets best performing neural network three hidden layers with andneurons trained forepochs with batch size ofThis design achieved accuracy offor smaller dataset accuracy offor larger dataset Figure : Normalized confusion matrixes test data from neural network classifier large dataset left smaller dataset right ADAPTIVE NETWORKBASED FUZZY INFERENCE SYSTEM ANFIS ANFIS combines structure neural network with “fuzzy logic” principles that attempt replicate nonlinearities medical professional’experiential knowledge when making decision about patients diagnosis based medical data This principle fuzzy logic looks classification more sliding scale betweenandwhere there more continuity subject’diagnosis heart disease instead binary classification ANFIS consists parameter types premise consequence which tuned during training algorithm There five layers ANFIS network structure first denoted fuzzification layer uses membership functions obtain fuzzy groups inputs Premise parameters tuned determine form membership functions second layer rule layer determines firing strengths rules generated first layer normalization layer comes third calculates normalized firing strengths belonging each rule fourth layer defuzzification layer weighted values each rule calculated using first order polynomial tuned consequence parameters fifth output layer then obtains actual output summing outputs from defuzzification layer ANFIS Layer Calculations Figure : True labels output ANFIS small dataset withfeatures Healthy subjects left subject with heart disease diagnosis right Figure : True labels output ANFIS small dataset withfeatures Healthy subjects left subject with heart disease diagnosis right Accuracy ANFIS determined utilizing same threshold binary classifiers which value greater thandenoted heart disease diagnosis vice versa Figureshows outputs ANFIS compared test data ANFIS performs quiet well small dataset achieving accuracy ofand performs with existing prediction methods larger dataset achieving accuracy ofThe small dataset ANFIS system tuned havemembership function rules utilized training learning rate ofwhile ANFIS trained using large dataset performed best withmembership function rules training learning rate ofLooking plots test data outputs small dataset appears that most misclassifications ANFIS were quite confident These particular data points should further studied better understand ANFIS confidently misclassifies them outputs larger dataset noisier their classifications which result smaller feature space CONCLUSION FUTURE WORK DATASET ANFIS SMALL/FEATURESLARGE/FEATURESGenerally performance good types classifiers tried best results achieving approximately % accuracy were ANFIS algorithm Knearest neighbors with =smaller dataset with more features While percent improvement from existing systems scoring just under % seem drastic this actually quite substantial disease that affects such high portion global population appears that either extra three features some other noncategorical attribute smaller dataset lead better performance heart disease prediction overall future work this could explored collecting more data inclusive three extra features running same experiments Other data interest could also collected analyzed future including lifestyle factors such diet smoking history body mass index frequency exercise geographic region While these introduce more levels uncertainty imprecision ANFIS able handle these sorts features better than traditional prediction approaches Further experiments ANFIS could also example would interesting incorporate medical professional’experiential knowledge initially suggesting certain membership rules fuzzification layer Lastly further exploration could done combining fuzzy logic principles with higher performing traditional classifiers such KNearest Neighbors Overall study successful achieving slight improvement over existing prediction methods heart disease diagnosis using same feature ANFIS performed well drastically better worse than traditional classification algorithms tested Future improvement will require further data collection with bigger feature space perhaps further understanding causes heart disease improve both feature selection membership function generation ANFIS algorithm Additionally since neural network classifier performed worst beneficial combining fuzzy logic principles with higher performing traditional classifiers type algorithm APPENDIX code found https://githubcom/ekunz/CSProject ACKNOWLEDGEMENTS/REFERENCESDua Graff CUCI Machine Learning Repository http://archiveicsuciedu/Irvine: University California School Information Computer ScienceIbrahim Olawale Funmilayo KDiagnosis Hepatitis using Adaptive NeuroFuzzy Inference System ANFIS International Journal Computer ApplicationsppDogantekin Dogantekin Avci Avci intelligent diagnosis system diabetes Linear Discriminant Analysis Adaptive Network Based Fuzzy Inference System: LDAANFIS Digital Signal ProcessingppWeng Reps Garibaldi Qureshi NCan Machinelearning improve cardiovascular risk prediction using routine clinical data? PLoS Sarkar “Hybrid model prediction heart disease” Soft Computing Brahim Zell “ANFISSNNS: Adaptive Network Fuzzy Inference System Stuttgart Neural Network Simulator” Fuzzy Systems Computer Science – 
Title,Content
Hierarchical image classification in CNNs,Hierarchical image classification CNNs Prashant Kumar Dated: June hierarchical classification improve classification accuracy neural networks when number categories large? explore this question construct neural network that classifies CIFAR database intocoarse categories Correlations between coarse classification probabilities filters another neural network then used perform fine classification intocategories three different approaches coarse category construction significant change accuracies compared baseline observed INTRODUCTION nation filters classification into fine categories possibly improving accuracy Further since fine classifi image classification problem prime cation layers independent each other applications convolutional neural networks CNNs disjoint coarse categories this project have tried simplest form single model used this three different approaches coarse category construc problem However when number possible cate tion gories large might useful provide partial clas code this project found insification images intermediate stage opposed endtoend learning might find attractive neural networks learn what information ARCHITECTURE BROAD vide However useful provide APPROACH input from human intuition learning this project want partial classification into coarse categories experiments done CIFAR image might useful This similar idea hierarchical database hasclasses withtraining images classification where image first classified into coarse class test additionalimages categories then into fine categories class Hierarchical classification architecture Architecture details: proposed architecture been introduced RefThey divided shown FigFor coarse neural network choose fine labels into coarse categories with certain fine architecture that classifies image intoclasses present multiple coarse categories image coarse categories main neural network choose first classified into categories then output this output Conv Layers corresponds classification into number independent fine filter activations after theconvo classification layers further classify these into fine lutional layers neural network then evaluates classes They train coarse classifier first based outer product these filters with coarse classifica preconstructed category hierarchy Then they train tion output Thus input given classification fineclassifier layers layers∗ dimensional overall output different note approach been used dimensional classification layers usualto perform classifcation images where different image linear fully connected layers Batch normalza classes visually very similar example identi tion been used after every convolutional layer Addi fying particular species bird from pictures tionally dropout layers added regularization birds filters convolutional layers before linear layers classification layers network combined bilinear This done base code CIFAR been taken from based intuition that different filters learn differentIn addition weight initialization been used features thus correlations between them should Refuseful classification method inspired this Outline training methodology: coarse approach network Figis first trained coarse clas this project investigate whether hierarchical clas sification training Then this trained coarse sification utilized help improve accuracy used while training main neural network fine classification layers Outline coarse category construction: independent HDCNN architecture However pare results from three different methods coarse might more useful have single network that egory construction first method category does fine classifications Therefore inspired erarchy constructed creatingcoarse categories bilinearCNN employ outer product between Each coarse category corresponds subset thethe coarse category probabilities filters fine labels Unlike RefI classify label into only convolutional layers neural network expected coarse category second method instead that each coarse category will choose different combi grouping various labels together training images added This called coarse layer serves pose projecting classes dimensions this fully connected recovery layer neurons added Thus this layer classifies dimensional data back into classes layers added this neural network trained with cross entropy loss FIGCoarse neural network trained perform classifica Thus this neural network maximizes recoverability tion intocoarse categories output combined with output after classes projected ontoactivations convolutional layers main neural network dimensions intuition behind this that visually through outer product perform final classification dissimilar categories easier distinguish thus intocategories easy recover recovery layer After training these last layers calculate acti vations dimensional coarse layer every image grouped together findcategories images regard training each thelabels find less their actual label third method full neuron that gets activated most theimages composite neural network model trained endtoend this label This neuron considered coarse without knowledge category hierarchy last class this label exact category hierarchy highly method tells whether coarse classification actu dependent random initialization pretraining ally useful first method named “Category Hierar However select that roughly balanced”second method “Coarse visual category” fine label distribution third method “EndtoEnd”found that using strong regularization with efficientfor weights recovery layer helped producing more balanced coarse categories This COARSE CATEGORY CONSTRUCTION done based understanding that regularization feature selector thus makes weight matrix this section describe detail coarse cate recovery layer sparse given fine label uses gories constructed each three methods relatively small number neurons from coarse layer work gets spread more uniformly over rons coarse layer Method : Constructing category hierarchy Method : Constructing coarse visual construct category hierarchy Refutilize categories spectral clustering confusion matrix trained neural network second method would disadvantage approach outlined coarse classes already defined CIFAR databse vious subsection that images corresponding have tried different approach given fine label might correspond same coarse category This observed during experimentation Thus construct coarse categories image image this case given image corresponds neuron that gets activated most coarse layer FigIn this images under same coarse class more visually similar than method Method : EndtoEnd trained neural FIGA fully connected coarse classification layer added network trained network project dimensional output dimensions dimensional fully connected recovery layer further added recover original classifi This doesn’require explicit coarse category cation struction trained endtoend hoped that this network will construct coarse categories itself First train neural network train that classifies training intocategories Then dimensional output TRAINING RESULTS network classified intocoarse categories using structure shown FigI modify trained Data preprocessing: training testing follows batch normalization layer images first normalized that mean each fullyconnected layer ofneurons with relu activation color channel zero standard deviation Further training flipped randomly hori Category Hierarchy zontal direction random crop×pixel size Trees chair forest maple tree motorcycle employed with padding ofpixels each edge tree orange pine tree snake willow tree worm TrainDev split: split thetraining images Vehicles cattle lawn mower mountain each label intotrainvalidation Validation palm tree pickup truck streetcar tank fixed chosen randomly every tractor train code apple leopard lion tiger wolf Optimation: training neural networks Carnivores initial leaning rate ofis used multiplied Categorybowl clock keyboard lamp plate factor ofif validation loss doesn’decrease raccoon rocket skunk forconsecutive steps regularization toHumans baby bicycle cloud flatfish girl stochastic gradient descent with momentumfor plain possum woman optimization algorithm Animals crab dinosaur dolphin lobster otter seal shark trout turtle whale Furniture/ road table television wardrobe straight lines? Baseline Categoryaquarium fish bear beaver camel chim panzee elephant hamster rose choose baseline obtained accuracies Insects beetle bottle butterfly caterpillar cockroach couch orchid pear poppy shown Table spider sunflower sweet pepper telephone tulip baseline CIFAR Categorybridge castle crocodile house kangaroo Dataset Accuracy lizard mouse mushroom porcupine Trainset % shrew skyscraper snail squirrel Validation % Test % TABLE TABLE Baseline network afterepochs neural network with architecture trained training accuracy quite compared classify training images into thesecoarse cate validation test accuracies This indicates high gories accuracy achieved coarse classification degree overfitting neural network shown Table Quite surprisingly Note: have avoided using pretrained accuracies were generally good coarse cate Figthe classification layers take input size∗gories that have human identifiable pattern They which available pretrained order shown Figavoid getting information from state trained neural networks decided selftrained work that results unbiased major trouble with this presence overfitting during training spoil coarse neural network classification Method : Category Hierarchy category hierarchy constructed neural work shown Table this step’baseline suffers from severe overfitting FIGIndividual coarse class accuracy coarse employ early stopping train another that Roughly human recognizable categories have better accura achieves % training % validation accu cies racies coarse categories seem very sensible example clearly notice severe overfitting coarse multiple types trees forest classified cate classification Since used training phase goryVehicles classified into categorySimilarly main neural network bound worsen general carnivores categoryhumans categorysea izability trained model animals fish categoryfurniture categoryThe main neural network trained endtoend insects categoryOther categories’seem with trained coarse neural network coarse have single identifiable pattern only speculate network frozen during this phase classification apple classified alongside cats accuracies mentioned Table IVCoarse accuracies model expected opposite because images Dataset Accuracy this coarse category construction supposed Trainset % more visually similar Also degree overfitting Validation % validation error were smaller coarse this case Test % reason could that recovery layer FigTABLE Coarse accuracy afterepochs overfitting Overall accuracies Dataset Accuracy Trainset % Validation % Test % Method : EndtoEnd TABLE Main accuracy afterepochs endtoend training slightly modified network architecture shown FigThe neural network achieves % higher accuracy coarse classification shouldn’require independent compared baseline validation However volutional layers Therefore activations noticed that doesn’generalize very well volutional layers passed through thefully connected test mentioned earlier reason this linear layers classify image intocoarse overfitting during coarse training phase Another categories These denoted coarse classification noticeable thing that doesn’overfit training figure This then used similar manner much baseline earlier compute correlations between convolutional minute improvement validation accuracy layer activations coarse classification leads speculate whether better trained coarse clas work trained endtoend sifier actually improve accuracy test Better training methods have used achieve this example following approach Refone this training coarse partially followed training main Then both coarse main fine tuned Method : Coarse Visual Categories FIGEndtoEnd architecture trainingvalidation used train coarse neural network accuracy obtained shown Table Activation last coarse classification layer been chosen softmax classification accuracies Coarse accuracies presented Table Dataset Accuracy Trainset % Validation % Test % Overall accuracies Dataset Accuracy TABLE Coarse accuracy afterepochs Trainset % Validation % Test % main neuralnetwork accuracies shown TABLE Main accuracy afterepochs Overall accuracies Dataset Accuracy Trainset % Upon analyzing coarse classification layers found Validation % that virtually images were classified into same Test % category obvious that this method doesn’build category hierarchy itself achieve this TABLE Main accuracy afterepochs loss term model that incentivizes classification images into different categories After trying multiple surprising that this performs worse than simpler loss functions tried using following asingle minibatch images size: purpose this course project:= =CONCLUSION FUTURE WORK = general didn’observe much improvement accu = oiracy these neural network architectures However =methods seems that avoiding overfitting −|||| + ||||  = coarse neural network might lead increase accuracy Better training methods need used this regard doing this training coarse +σcand main cycles could train just coarse=small number epochs then train main small number epochs This would constitute training cycle After repeating multiple cycles could train both them together achieve fully trained network different approach than employed this project where represents coarse class image “ ” would implement filterfilter correlations minibatch represents dimensional output within coarse category suggested RefOne coarse layers Intuitively this loss function incen doing would consider filterfilter outer tivizes clustering between images that visually simi product followed projection onto smaller number first term Mexican potential that dimensions then this followed outer motes bigger alignment coarse classification along product with coarse classification probabilities maximally activated neuron second term motes smaller overlap between images corresponding ferent coarse classes haven’achieved much success ACKNOWLEDGMENTS this retrospect spent time using above loss function different project addition great teaching team course supervised image classification CIFAR think benefited from discussions with friends Shubham simpler loss function like following could have worked Toshiniwal Palak BhushanTsungYu Aruni Chowdhury SubhransuIEEE International Conference Computer Vision Maji Bilinear Models Finegrained Visual Recog ICCVSantiago Chile December pages nition Proceedings IEEE International Conference –Computer Vision pages –https://githubcom/prashantkumar/hicCNN Zhang Piramuthu Jagadeesh DeCoste https://githubcom/kuangliu/pytorchcifarW HDCNN: hierarchical deep convolu https://gistgithubcom/jeasinema/ tional neural networks large scale visual recognition edcecefaffafff 
Title,Content
DIAGNOSIS OF HEART DISEASE USING ADAPTIVE NETWORK-BASED,DIAGNOSIS HEART DISEASE USING ADAPTIVE NETWORKBASED FUZZY INFERENCE SYSTEM ANFIS ERIN KUNZ CSSPRINGABSTRACT Heart Disease leading cause death both women United States Approximatelypeople from heart disease each year which equivalent toin everydeaths Many health risk lifestyle factors have been associated with onset heart disease including high blood pressure high cholesterol smoking physical activity levels sleep quality stress much more Most prevention treatment techniques focused these such lifestyle factors this reason diagnosis prognosis somewhat subjective despite clinically available quantifications that more indicative heart disease this study variety machine learning techniques used predict occurrence heart disease given variety easily obtainable hearthealthrelated features Furthermore incorporate more subjective nonlinear decisionmaking that medical professional might offer likelihood that heart disease prediction involve complex relationships between features further analysis utilizing fuzzy inference system done INTRODUCTION RELATED WORK More thanAmericans from heart disease every year leading cause death globally more common been increasing many developed countries particularly United States variety risk factors linked heart disease including diet exercises tobacco alcohol high blood pressure estimated that % heart disease preventable through improving these risk factors Cardiovascular heart disease actually refers class diseases that involve heart blood vessels includes several distinct types – coronary artery diseases stroke heart failure hypertensive heart failure rheumatic heart disease many more this reason diagnosis even less linear based lifestyle factors available medical data Being able better predict prevent heart disease could change save lives millions people variety machine learning classification techniques able better analyze more complex relationships between medical data features possible heart disease diagnosis Several studies have been completed define classifier heart disease utilizing machine learning techniques init been proven that machine learning techniques such logistic regression neural networks improve upon established risk prediction methods diagnosing heart disease Ultimately prediction methods achieved accuracy nearing % While great improvement from current medical standard with heart disease affecting such high portion population misdiagnosis rate higher than % still huge portion population improving this accuracy rate even further could positively impact possibly save many more lives Additionally many studies have been done predicting diagnosis other lifestylecorrelated illnesses utilizing fuzzy logic principles Inference systems utilizing fuzzy logic have been used with success predictions both Diabetes Hepatitis Intuitively continuous scale betweenandmakes sense diseases like heart disease diabetes hepatitis known gradual development result longterm lifestyle factors usually aren’instantaneous occurrences this study input willfeatures medical data that easily obtainable typically used heart disease diagnosis output will prediction heart diseaseor heart diseaseWe will tune variety classification algorithms achieve best accuracy possible Furthermore this study also attempt incorporate nonlinear complexities through merging fuzzy logic principles with artificial neural network Adaptive NetworkBased Fuzzy Inference System seeking further improvement DATASET FEATURES this study datasets were considered first complete dataset ofsubjects withfeatures including chest pain type resting blood pressure cholesterol fasting blood sugar resting electrocardiographic results exercise induced angina depression induced exercise relative rest slope peak exercise segment number major vessels colored flourosopy presence defect reversible defect fixed defect diagnosis heart diseaseorThe second dataset ofsubjects with same firstfeatures first dataset diagnosis heart disease This dataset collected from studies Hungary Switzerland Cleveland Both these datasets were obtained from Machine Learning RepositoryAny missing feature entries datasets were imputed with mean values Initially looking correlation matrix dataset withfeatures dataset withfeatures appears there strong correlation between feature diagnosis heart disease target Figure : Correlation matrix forfeatures combined datasets left correlation matrix allfeatures first dataset right METHODS EXPERIMENTS RESULTS Four types binary classifiers were initially tested both datasets: logistic regression KNearest Neighbor classifier Support Vector Machine classifier artificial neural network experiments data randomly shuffled normalized split % training % testing accuracies reported test data LOGISTIC REGRESSION Initially logistic regression applied both datasets Logistic regression nonlinear transformation linear regression model between input variables binary class assignment maps this linear regression using function like sigmoid shape that probabilities mapped betweenandThen class input example determined based this probability value greater thanwould denote classand vice versa first phase classifier both datasets logistic regression strategy fairly well with over % accuracy both datasets larger dataset withfeatures achieved accuracy ofand smaller dataset withfeatures achieved accuracy ofThere nearly % improvement between larger dataset with less features than smaller dataset with more features possibly indicating that three additional features provide additional relevant information diagnosis heart disease However smaller size dataset more experiments will need confidently make this conclusion this could also result overfitting this particular dataset region collection KNEIGHBORS CLASSIFIER Next Knearest neighbors classifier both datasets this type classifier nearest neighbors given data point analyzed majority class these neighbors assigned data point number nearest neighbors varied this study varied fromtofor larger dataset finding peak accuracy ofat= nearest neighbors smaller dataset varied fromtofinding peak accuracy offor= nearest neighbors both datasets accuracy classifier improved from logistic regression classifier Again slightly superior performance from smaller dataset with more features than larger dataset with three fewer features Figure : Score plots variations neighbors small dataset larger dataset bottom SUPPORT VECTOR MACHINES Next support vector machine classifier tested accuracy could further improved Support vector machines machine learning technique that looks separations data when mapped higherdimensional space utilize variety “kernel tricks” assess just linear relationships also polynomial radial basis function sigmoid feature mappings find split higher dimensional space between binary classifications this study linear polynomial radial basis function sigmoid feature mappings were tested both datasets Figure : Score plots different feature mapping classifier large dataset left smaller dataset right classifier does improve upon theoraccuracy from highest performing Kneighbors algorithm however interesting note that different feature mappings different performance orders datasets first time classifier larger dataset with fewer features outperformed classifier smaller dataset with more features radial basis function feature large dataset classifier performed best overall with accuracy ofwhile sigmoid function performed best smaller dataset with accuracy ofThis indicates that three additional features smaller dataset complexities relationships between classes higher dimensional space that able entirely captured these four feature mapping split strategies NEURAL NETWORK next experiment implement neural network classifier datasets could exceed previous accuracy ofSeveral iterations neural network were constructed varying numbers hidden layers numbers neurons each layer batch size during training hidden input layers used ReLu activation function output layer employed sigmoid activation function both datasets best performing neural network three hidden layers with andneurons trained forepochs with batch size ofThis design achieved accuracy offor smaller dataset accuracy offor larger dataset Figure : Normalized confusion matrixes test data from neural network classifier large dataset left smaller dataset right ADAPTIVE NETWORKBASED FUZZY INFERENCE SYSTEM ANFIS ANFIS combines structure neural network with “fuzzy logic” principles that attempt replicate nonlinearities medical professional’experiential knowledge when making decision about patients diagnosis based medical data This principle fuzzy logic looks classification more sliding scale betweenandwhere there more continuity subject’diagnosis heart disease instead binary classification ANFIS consists parameter types premise consequence which tuned during training algorithm There five layers ANFIS network structure first denoted fuzzification layer uses membership functions obtain fuzzy groups inputs Premise parameters tuned determine form membership functions second layer rule layer determines firing strengths rules generated first layer normalization layer comes third calculates normalized firing strengths belonging each rule fourth layer defuzzification layer weighted values each rule calculated using first order polynomial tuned consequence parameters fifth output layer then obtains actual output summing outputs from defuzzification layer ANFIS Layer Calculations Figure : True labels output ANFIS small dataset withfeatures Healthy subjects left subject with heart disease diagnosis right Figure : True labels output ANFIS small dataset withfeatures Healthy subjects left subject with heart disease diagnosis right Accuracy ANFIS determined utilizing same threshold binary classifiers which value greater thandenoted heart disease diagnosis vice versa Figureshows outputs ANFIS compared test data ANFIS performs quiet well small dataset achieving accuracy ofand performs with existing prediction methods larger dataset achieving accuracy ofThe small dataset ANFIS system tuned havemembership function rules utilized training learning rate ofwhile ANFIS trained using large dataset performed best withmembership function rules training learning rate ofLooking plots test data outputs small dataset appears that most misclassifications ANFIS were quite confident These particular data points should further studied better understand ANFIS confidently misclassifies them outputs larger dataset noisier their classifications which result smaller feature space CONCLUSION FUTURE WORK DATASET ANFIS SMALL/FEATURESLARGE/FEATURESGenerally performance good types classifiers tried best results achieving approximately % accuracy were ANFIS algorithm Knearest neighbors with =smaller dataset with more features While percent improvement from existing systems scoring just under % seem drastic this actually quite substantial disease that affects such high portion global population appears that either extra three features some other noncategorical attribute smaller dataset lead better performance heart disease prediction overall future work this could explored collecting more data inclusive three extra features running same experiments Other data interest could also collected analyzed future including lifestyle factors such diet smoking history body mass index frequency exercise geographic region While these introduce more levels uncertainty imprecision ANFIS able handle these sorts features better than traditional prediction approaches Further experiments ANFIS could also example would interesting incorporate medical professional’experiential knowledge initially suggesting certain membership rules fuzzification layer Lastly further exploration could done combining fuzzy logic principles with higher performing traditional classifiers such KNearest Neighbors Overall study successful achieving slight improvement over existing prediction methods heart disease diagnosis using same feature ANFIS performed well drastically better worse than traditional classification algorithms tested Future improvement will require further data collection with bigger feature space perhaps further understanding causes heart disease improve both feature selection membership function generation ANFIS algorithm Additionally since neural network classifier performed worst beneficial combining fuzzy logic principles with higher performing traditional classifiers type algorithm APPENDIX code found https://githubcom/ekunz/CSProject ACKNOWLEDGEMENTS/REFERENCESDua Graff CUCI Machine Learning Repository http://archiveicsuciedu/Irvine: University California School Information Computer ScienceIbrahim Olawale Funmilayo KDiagnosis Hepatitis using Adaptive NeuroFuzzy Inference System ANFIS International Journal Computer ApplicationsppDogantekin Dogantekin Avci Avci intelligent diagnosis system diabetes Linear Discriminant Analysis Adaptive Network Based Fuzzy Inference System: LDAANFIS Digital Signal ProcessingppWeng Reps Garibaldi Qureshi NCan Machinelearning improve cardiovascular risk prediction using routine clinical data? PLoS Sarkar “Hybrid model prediction heart disease” Soft Computing Brahim Zell “ANFISSNNS: Adaptive Network Fuzzy Inference System Stuttgart Neural Network Simulator” Fuzzy Systems Computer Science – 
